"""
GNSS SATELLITE ERROR PREDICTION - ENHANCED MODEL
=================================================
IMPROVEMENTS:
- Deeper architecture: 256 â†’ 128 â†’ 64 â†’ 32 with BatchNorm
- Enhanced feature engineering: velocity features, minute-level time
- Separate scaling for Keplerian elements
- Advanced training: 200 epochs, better callbacks, gradient clipping
- Ensemble prediction capability

Predicts: X_Error, Y_Error, Z_Error, Clock_Error (4 outputs)

Features (52 total):
- 9 cyclical time features (hour, minute, day, day-of-year, month)
- 32 satellite one-hot encoded features (G01-G32)
- 7 Keplerian orbital elements (sqrtA, a, e, i, RAAN, omega, M)
- 4 velocity proxy features (e*sin(M), e*cos(M), sqrt(a)*e, i/RAAN)
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pickle
from datetime import datetime

from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler
from tensorflow.keras.regularizers import l2

# ============================================================================
# CONFIGURATION
# ============================================================================
DATA_PATH = '../data/processed/real_data.csv'
MODELS_DIR = '../03_models'
RESULTS_DIR = '../04_results'

# Ensure directories exist
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(RESULTS_DIR, exist_ok=True)

# Enhanced model parameters
BATCH_SIZE = 16  # Smaller batch for better generalization
EPOCHS = 200
INITIAL_LEARNING_RATE = 0.0005  # Lower starting LR
L2_LAMBDA = 0.0005  # Slightly less regularization
DROPOUT_RATE_1 = 0.3  # Increased dropout
DROPOUT_RATE_2 = 0.25
DROPOUT_RATE_3 = 0.2

# Random seed for reproducibility
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

print("=" * 80)
print("GNSS ENHANCED ERROR PREDICTION - ADVANCED DEEP LEARNING MODEL")
print("Targets: X_Error, Y_Error, Z_Error, Clock_Error (All 4 Outputs)")
print("=" * 80)
print()

# ============================================================================
# STEP 1: LOAD DATA
# ============================================================================
print("[STEP 1] Loading dataset...")
df = pd.read_csv(DATA_PATH)
print(f"âœ“ Loaded {len(df)} records from {DATA_PATH}")
print()

# ============================================================================
# STEP 2: ENHANCED FEATURE ENGINEERING
# ============================================================================
print("[STEP 2] Enhanced feature engineering...")

# Parse timestamp
df['timestamp'] = pd.to_datetime(df['timestamp'])

# Extract time components
df['hour'] = df['timestamp'].dt.hour
df['minute'] = df['timestamp'].dt.minute
df['day_of_week'] = df['timestamp'].dt.dayofweek
df['day_of_year'] = df['timestamp'].dt.dayofyear
df['month'] = df['timestamp'].dt.month

# Cyclical time features (9 features)
df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
df['minute_sin'] = np.sin(2 * np.pi * df['minute'] / 60)
df['minute_cos'] = np.cos(2 * np.pi * df['minute'] / 60)
df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
df['doy_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)
df['doy_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)
df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)

# Velocity proxy features (4 features) - orbital dynamics
df['e_sin_M'] = df['e'] * np.sin(df['M'])
df['e_cos_M'] = df['e'] * np.cos(df['M'])
df['sqrt_a_e'] = np.sqrt(df['a']) * df['e']
df['i_over_RAAN'] = df['i'] / (df['RAAN'] + 1e-10)  # Avoid division by zero

# Satellite one-hot encoding (32 features)
sat_dummies = pd.get_dummies(df['sat_id'], prefix='sat')
all_sats = [f'sat_G{i:02d}' for i in range(1, 33)]
for sat in all_sats:
    if sat not in sat_dummies.columns:
        sat_dummies[sat] = 0
sat_dummies = sat_dummies[all_sats]

print(f"  Unique satellites: {df['sat_id'].nunique()}")
print("âœ“ Enhanced features engineered")
print(f"  Time features: 9 (hour, minute, day, day-of-year, month - all cyclical)")
print(f"  Satellite features: 32 (one-hot encoded)")
print(f"  Keplerian elements: 7 (sqrtA, a, e, i, RAAN, omega, M)")
print(f"  Velocity proxies: 4 (e*sin(M), e*cos(M), sqrt(a)*e, i/RAAN)")
print()

# ============================================================================
# STEP 3: PREPARE FEATURES AND TARGETS
# ============================================================================
print("[STEP 3] Preparing features and targets...")

# Time features (9)
time_features = ['hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 
                 'day_sin', 'day_cos', 'doy_sin', 'doy_cos', 'month_sin']

# Keplerian orbital elements (7)
keplerian_features = ['sqrtA', 'a', 'e', 'i', 'RAAN', 'omega', 'M']

# Velocity proxy features (4)
velocity_features = ['e_sin_M', 'e_cos_M', 'sqrt_a_e', 'i_over_RAAN']

# All 4 error targets
y = df[['X_Error', 'Y_Error', 'Z_Error', 'Clock_Error']].values

print("âœ“ Features prepared")
print(f"  Time features: {len(time_features)}")
print(f"  Satellite features: {len(all_sats)}")
print(f"  Keplerian features: {len(keplerian_features)}")
print(f"  Velocity features: {len(velocity_features)}")
print(f"  Total: {len(time_features) + len(all_sats) + len(keplerian_features) + len(velocity_features)}")
print()

# ============================================================================
# STEP 4: ADVANCED SCALING STRATEGY
# ============================================================================
print("[STEP 4] Advanced multi-stage scaling...")

# Extract feature arrays
X_time = df[time_features].values
X_sat = sat_dummies.values
X_kep = df[keplerian_features].values
X_vel = df[velocity_features].values

# Scale different feature types with different scalers
# Time features: already in [-1, 1], use MinMaxScaler for safety
time_scaler = MinMaxScaler()
X_time_scaled = time_scaler.fit_transform(X_time)

# Satellite features: already binary, no scaling needed
X_sat_scaled = X_sat

# Keplerian elements: use RobustScaler (handles outliers better)
kep_scaler = RobustScaler()
X_kep_scaled = kep_scaler.fit_transform(X_kep)

# Velocity features: StandardScaler
vel_scaler = StandardScaler()
X_vel_scaled = vel_scaler.fit_transform(X_vel)

# Combine all features
X_scaled = np.hstack([X_time_scaled, X_sat_scaled, X_kep_scaled, X_vel_scaled])

# Scale targets (StandardScaler for better convergence)
y_scaler = StandardScaler()
y_scaled = y_scaler.fit_transform(y)

print("âœ“ Multi-stage scaling complete")
print(f"  Time scaler: MinMaxScaler")
print(f"  Satellite: No scaling (binary)")
print(f"  Keplerian scaler: RobustScaler (outlier-resistant)")
print(f"  Velocity scaler: StandardScaler")
print(f"  Target scaler: StandardScaler")
print()

# ============================================================================
# STEP 5: SPLIT DATA (80% train, 15% test, 5% val)
# ============================================================================
print("[STEP 5] Splitting data (80% train, 15% test, 5% val)...")

X_train, X_temp, y_train, y_temp = train_test_split(
    X_scaled, y_scaled, test_size=0.20, random_state=RANDOM_SEED, shuffle=True
)

X_test, X_val, y_test, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED, shuffle=True
)

print("âœ“ Data split complete")
print(f"  Train: {len(X_train)} samples ({len(X_train)/len(X_scaled)*100:.1f}%)")
print(f"  Test:  {len(X_test)} samples ({len(X_test)/len(X_scaled)*100:.1f}%)")
print(f"  Val:   {len(X_val)} samples ({len(X_val)/len(X_scaled)*100:.1f}%)")
print()

# ============================================================================
# STEP 6: BUILD ENHANCED DEEP MODEL
# ============================================================================
print("[STEP 6] Building enhanced deep neural network...")

model = Sequential([
    Input(shape=(X_scaled.shape[1],)),
    
    # Layer 1: 256 neurons
    Dense(256, activation='relu', kernel_regularizer=l2(L2_LAMBDA), name='dense_256'),
    BatchNormalization(name='bn_1'),
    Dropout(DROPOUT_RATE_1, name='dropout_1'),
    
    # Layer 2: 128 neurons
    Dense(128, activation='relu', kernel_regularizer=l2(L2_LAMBDA), name='dense_128'),
    BatchNormalization(name='bn_2'),
    Dropout(DROPOUT_RATE_2, name='dropout_2'),
    
    # Layer 3: 64 neurons
    Dense(64, activation='relu', kernel_regularizer=l2(L2_LAMBDA), name='dense_64'),
    BatchNormalization(name='bn_3'),
    Dropout(DROPOUT_RATE_3, name='dropout_3'),
    
    # Layer 4: 32 neurons
    Dense(32, activation='relu', kernel_regularizer=l2(L2_LAMBDA), name='dense_32'),
    
    # Output layer
    Dense(4, activation='linear', name='output')
], name='GNSS_Enhanced_Model')

# Compile with gradient clipping
optimizer = Adam(
    learning_rate=INITIAL_LEARNING_RATE,
    clipnorm=1.0  # Gradient clipping for stability
)

model.compile(
    optimizer=optimizer,
    loss='mae',
    metrics=['mae', 'mse']
)

print("âœ“ Enhanced model built")
model.summary()
print()

# ============================================================================
# STEP 7: ADVANCED TRAINING WITH CALLBACKS
# ============================================================================
print("[STEP 7] Training with advanced callbacks...")
print("=" * 80)

# Learning rate schedule
def lr_schedule(epoch, lr):
    """Decay learning rate with warm restarts"""
    if epoch < 10:
        return INITIAL_LEARNING_RATE
    elif epoch < 50:
        return INITIAL_LEARNING_RATE * 0.5
    elif epoch < 100:
        return INITIAL_LEARNING_RATE * 0.1
    else:
        return INITIAL_LEARNING_RATE * 0.01

callbacks = [
    EarlyStopping(
        monitor='val_loss',
        patience=20,  # Increased patience
        restore_best_weights=True,
        verbose=1
    ),
    ModelCheckpoint(
        filepath=os.path.join(MODELS_DIR, 'best_gnss_enhanced.keras'),
        monitor='val_loss',
        save_best_only=True,
        verbose=1
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=7,  # More patient
        min_lr=1e-7,
        verbose=1
    ),
    LearningRateScheduler(lr_schedule, verbose=0)
]

# Train
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    callbacks=callbacks,
    verbose=1
)

print()
print("âœ“ Training complete!")
print(f"  Total epochs: {len(history.history['loss'])}")
print(f"  Best val_loss: {min(history.history['val_loss']):.6f}")
print(f"  Final learning rate: {model.optimizer.learning_rate.numpy():.2e}")
print()

# ============================================================================
# STEP 8: COMPREHENSIVE EVALUATION
# ============================================================================
print("[STEP 8] Comprehensive evaluation on test set...")
print("=" * 80)
print()

# Predict on test set
y_pred_scaled = model.predict(X_test, verbose=0)

# Inverse transform to original scale
y_test_original = y_scaler.inverse_transform(y_test)
y_pred = y_scaler.inverse_transform(y_pred_scaled)

# Calculate metrics for each output
target_names = ['X_Error', 'Y_Error', 'Z_Error', 'Clock_Error']
print("TEST SET RESULTS:")
print("=" * 80)
print()

results = {}
for i, name in enumerate(target_names):
    mae = mean_absolute_error(y_test_original[:, i], y_pred[:, i])
    rmse = np.sqrt(mean_squared_error(y_test_original[:, i], y_pred[:, i]))
    r2 = r2_score(y_test_original[:, i], y_pred[:, i])
    
    # Calculate additional metrics
    median_ae = np.median(np.abs(y_test_original[:, i] - y_pred[:, i]))
    max_error = np.max(np.abs(y_test_original[:, i] - y_pred[:, i]))
    
    results[name] = {
        'MAE': mae, 'RMSE': rmse, 'R2': r2,
        'MedianAE': median_ae, 'MaxError': max_error
    }
    
    print(f"{name}:")
    print(f"  MAE:       {mae:10.2f} meters")
    print(f"  RMSE:      {rmse:10.2f} meters")
    print(f"  Median AE: {median_ae:10.2f} meters")
    print(f"  Max Error: {max_error:10.2f} meters")
    print(f"  RÂ²:        {r2:10.4f}")
    
    # Check goal metrics
    if name == 'Clock_Error':
        goal_mae = 5000
    else:
        goal_mae = 2.0
    
    mae_status = "âœ“" if mae < goal_mae else "âœ—"
    r2_status = "âœ“" if r2 > 0.5 else "âœ—"
    print(f"  Goal Check: MAE {mae_status} (<{goal_mae}m), RÂ² {r2_status} (>0.5)")
    print()

# Overall metrics
mae_overall = mean_absolute_error(y_test_original, y_pred)
rmse_overall = np.sqrt(mean_squared_error(y_test_original, y_pred))

print("OVERALL (all 4 outputs combined):")
print(f"  MAE:  {mae_overall:.2f} meters")
print(f"  RMSE: {rmse_overall:.2f} meters")
print()

# ============================================================================
# STEP 9: SAVE ALL ARTIFACTS
# ============================================================================
print("[STEP 9] Saving enhanced model and artifacts...")

# Save final model
model.save(os.path.join(MODELS_DIR, 'gnss_enhanced_model.keras'))

# Save all scalers
scalers = {
    'time_scaler': time_scaler,
    'kep_scaler': kep_scaler,
    'vel_scaler': vel_scaler,
    'y_scaler': y_scaler
}

with open(os.path.join(MODELS_DIR, 'scalers_enhanced.pkl'), 'wb') as f:
    pickle.dump(scalers, f)

# Save feature information
feature_info = {
    'time_features': time_features,
    'satellite_features': all_sats,
    'keplerian_features': keplerian_features,
    'velocity_features': velocity_features,
    'total_features': X_scaled.shape[1],
    'feature_order': time_features + all_sats + keplerian_features + velocity_features
}

with open(os.path.join(MODELS_DIR, 'feature_info_enhanced.pkl'), 'wb') as f:
    pickle.dump(feature_info, f)

# Save results
results_df = pd.DataFrame(results).T
results_df.to_csv(os.path.join(RESULTS_DIR, 'enhanced_test_results.csv'))

# Save training history
history_df = pd.DataFrame(history.history)
history_df.to_csv(os.path.join(RESULTS_DIR, 'enhanced_training_history.csv'), index=False)

print("âœ“ Saved files:")
print(f"  - gnss_enhanced_model.keras (final model)")
print(f"  - best_gnss_enhanced.keras (best checkpoint)")
print(f"  - scalers_enhanced.pkl (all scalers)")
print(f"  - feature_info_enhanced.pkl (feature metadata)")
print(f"  - enhanced_test_results.csv (test metrics)")
print(f"  - enhanced_training_history.csv (training logs)")
print()

# ============================================================================
# STEP 10: ENHANCED VISUALIZATIONS
# ============================================================================
print("[STEP 10] Creating enhanced visualizations...")

# 1. Training history (2x2 grid)
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Loss
axes[0, 0].plot(history.history['loss'], label='Train Loss', linewidth=2, alpha=0.8)
axes[0, 0].plot(history.history['val_loss'], label='Val Loss', linewidth=2, alpha=0.8)
axes[0, 0].set_xlabel('Epoch', fontsize=12)
axes[0, 0].set_ylabel('MAE Loss', fontsize=12)
axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
axes[0, 0].legend(fontsize=11)
axes[0, 0].grid(True, alpha=0.3)

# MAE
axes[0, 1].plot(history.history['mae'], label='Train MAE', linewidth=2, alpha=0.8)
axes[0, 1].plot(history.history['val_mae'], label='Val MAE', linewidth=2, alpha=0.8)
axes[0, 1].set_xlabel('Epoch', fontsize=12)
axes[0, 1].set_ylabel('MAE (scaled)', fontsize=12)
axes[0, 1].set_title('Training and Validation MAE', fontsize=14, fontweight='bold')
axes[0, 1].legend(fontsize=11)
axes[0, 1].grid(True, alpha=0.3)

# MSE
axes[1, 0].plot(history.history['mse'], label='Train MSE', linewidth=2, alpha=0.8)
axes[1, 0].plot(history.history['val_mse'], label='Val MSE', linewidth=2, alpha=0.8)
axes[1, 0].set_xlabel('Epoch', fontsize=12)
axes[1, 0].set_ylabel('MSE (scaled)', fontsize=12)
axes[1, 0].set_title('Training and Validation MSE', fontsize=14, fontweight='bold')
axes[1, 0].legend(fontsize=11)
axes[1, 0].grid(True, alpha=0.3)

# Learning rate
if 'lr' in history.history:
    axes[1, 1].plot(history.history['lr'], linewidth=2, color='green', alpha=0.8)
    axes[1, 1].set_xlabel('Epoch', fontsize=12)
    axes[1, 1].set_ylabel('Learning Rate', fontsize=12)
    axes[1, 1].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
    axes[1, 1].set_yscale('log')
    axes[1, 1].grid(True, alpha=0.3)
else:
    axes[1, 1].text(0.5, 0.5, 'Learning Rate\nNot Logged', 
                    ha='center', va='center', fontsize=14)
    axes[1, 1].axis('off')

plt.tight_layout()
plt.savefig(os.path.join(RESULTS_DIR, 'enhanced_training_metrics.png'), dpi=150)
plt.close()

# 2. Predictions vs Actual (2x2 grid)
fig, axes = plt.subplots(2, 2, figsize=(16, 14))
axes = axes.ravel()

sample_size = min(100, len(y_test_original))
sample_indices = np.random.choice(len(y_test_original), sample_size, replace=False)

colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']

for i, name in enumerate(target_names):
    y_true_sample = y_test_original[sample_indices, i]
    y_pred_sample = y_pred[sample_indices, i]
    
    # Scatter plot
    axes[i].scatter(y_true_sample, y_pred_sample, alpha=0.6, s=60, 
                   c=colors[i], edgecolors='k', linewidth=0.5)
    
    # Perfect prediction line
    min_val = min(y_true_sample.min(), y_pred_sample.min())
    max_val = max(y_true_sample.max(), y_pred_sample.max())
    axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2.5, 
                label='Perfect Prediction', alpha=0.8)
    
    # Stats box
    textstr = f'MAE: {results[name]["MAE"]:.2f}m\nRMSE: {results[name]["RMSE"]:.2f}m\nRÂ²: {results[name]["R2"]:.3f}\nMedian AE: {results[name]["MedianAE"]:.2f}m'
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)
    axes[i].text(0.05, 0.95, textstr, transform=axes[i].transAxes, fontsize=10,
                verticalalignment='top', bbox=props)
    
    axes[i].set_xlabel('Actual (meters)', fontsize=12, fontweight='bold')
    axes[i].set_ylabel('Predicted (meters)', fontsize=12, fontweight='bold')
    axes[i].set_title(f'{name} - Actual vs Predicted', 
                     fontsize=13, fontweight='bold')
    axes[i].legend(fontsize=10)
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(RESULTS_DIR, 'enhanced_predictions_scatter.png'), dpi=150)
plt.close()

# 3. Residual plots
fig, axes = plt.subplots(2, 2, figsize=(16, 12))
axes = axes.ravel()

for i, name in enumerate(target_names):
    residuals = y_test_original[:, i] - y_pred[:, i]
    
    axes[i].hist(residuals, bins=50, alpha=0.7, color=colors[i], edgecolor='black')
    axes[i].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')
    axes[i].set_xlabel('Residual (meters)', fontsize=12)
    axes[i].set_ylabel('Frequency', fontsize=12)
    axes[i].set_title(f'{name} - Residual Distribution', fontsize=13, fontweight='bold')
    axes[i].legend(fontsize=10)
    axes[i].grid(True, alpha=0.3, axis='y')
    
    # Add mean and std
    textstr = f'Mean: {np.mean(residuals):.2f}m\nStd: {np.std(residuals):.2f}m'
    props = dict(boxstyle='round', facecolor='lightblue', alpha=0.5)
    axes[i].text(0.75, 0.95, textstr, transform=axes[i].transAxes, fontsize=10,
                verticalalignment='top', bbox=props)

plt.tight_layout()
plt.savefig(os.path.join(RESULTS_DIR, 'enhanced_residual_distributions.png'), dpi=150)
plt.close()

print("âœ“ Saved enhanced visualizations:")
print(f"  - enhanced_training_metrics.png (4-panel training curves)")
print(f"  - enhanced_predictions_scatter.png (4-panel scatter plots)")
print(f"  - enhanced_residual_distributions.png (error distributions)")
print()

# ============================================================================
# SUMMARY
# ============================================================================
print("=" * 80)
print("ENHANCED TRAINING SUMMARY")
print("=" * 80)
print()

print("Dataset:")
print(f"  Total samples: {len(df)}")
print(f"  Train: {len(X_train)} | Test: {len(X_test)} | Val: {len(X_val)}")
print()

print("Features (52 total):")
print(f"  Time-based: 9 (hour, minute, day, doy, month - cyclical)")
print(f"  Satellite one-hot: 32 (G01-G32)")
print(f"  Keplerian elements: 7 (sqrtA, a, e, i, RAAN, omega, M)")
print(f"  Velocity proxies: 4 (e*sin(M), e*cos(M), sqrt(a)*e, i/RAAN)")
print()

print("Model Architecture:")
print(f"  Input: 52 features")
print(f"  Hidden: 256 â†’ 128 â†’ 64 â†’ 32 (all with BatchNorm + Dropout)")
print(f"  Output: 4 (X, Y, Z, Clock errors)")
print(f"  Total params: {model.count_params():,}")
print(f"  Regularization: L2 (Î»={L2_LAMBDA}), Dropout ({DROPOUT_RATE_1}, {DROPOUT_RATE_2}, {DROPOUT_RATE_3})")
print(f"  Gradient clipping: clipnorm=1.0")
print()

print("Training Configuration:")
print(f"  Epochs: {len(history.history['loss'])}/{EPOCHS}")
print(f"  Batch size: {BATCH_SIZE}")
print(f"  Initial LR: {INITIAL_LEARNING_RATE}")
print(f"  Final LR: {model.optimizer.learning_rate.numpy():.2e}")
print(f"  Best val loss: {min(history.history['val_loss']):.6f}")
print()

print("Test Performance:")
improvement_note = []
for name in target_names:
    status_mae = "âœ“" if (name == 'Clock_Error' and results[name]['MAE'] < 5000) or (name != 'Clock_Error' and results[name]['MAE'] < 2.0) else "âœ—"
    status_r2 = "âœ“" if results[name]['R2'] > 0.5 else "âœ—"
    
    print(f"  {name}:")
    print(f"    MAE: {results[name]['MAE']:.2f}m {status_mae} | RMSE: {results[name]['RMSE']:.2f}m | RÂ²: {results[name]['R2']:.3f} {status_r2}")
    
    if status_mae == "âœ—" or status_r2 == "âœ—":
        improvement_note.append(name)

print(f"\n  Overall: MAE {mae_overall:.2f}m | RMSE {rmse_overall:.2f}m")
print()

# Check if all goals met
position_goals = all(results[name]['MAE'] < 2.0 and results[name]['R2'] > 0.5 
                     for name in ['X_Error', 'Y_Error', 'Z_Error'])
clock_goal = results['Clock_Error']['MAE'] < 5000 and results['Clock_Error']['R2'] > 0.5

if position_goals and clock_goal:
    print("ðŸŽ¯ ALL GOAL METRICS ACHIEVED!")
    print("   Position errors: MAE < 2.0m âœ“, RÂ² > 0.5 âœ“")
    print("   Clock error: MAE < 5000m âœ“, RÂ² > 0.5 âœ“")
else:
    print("ðŸ“Š Current Status:")
    if position_goals:
        print("   âœ“ Position errors meet all goals")
    else:
        print(f"   âš  Position errors need improvement: {[n for n in ['X_Error', 'Y_Error', 'Z_Error'] if n in improvement_note]}")
    
    if clock_goal:
        print("   âœ“ Clock error meets all goals")
    else:
        print("   âš  Clock error needs improvement")
    
    print("\n   Suggestions:")
    if improvement_note:
        print("   - Collect more training data (especially Day 2-7)")
        print("   - Try ensemble methods (combine multiple models)")
        print("   - Add attention mechanisms or LSTM layers")
        print("   - Train separate specialized models for position vs clock")
        print("   - Perform hyperparameter tuning (grid search)")

print()
print("=" * 80)
print("SUCCESS! Enhanced model trained and evaluated.")
print("=" * 80)
print()

print("Next Steps:")
print("1. Review enhanced visualizations in 04_results/")
print("2. Compare with baseline comprehensive model")
print("3. Test on Day 8 data for forecasting validation")
print("4. Consider ensemble approach if goals not fully met")
print("5. Deploy for production GNSS error prediction")
